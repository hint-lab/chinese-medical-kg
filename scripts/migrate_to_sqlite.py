#!/usr/bin/env python3
"""
å°†JSONæ•°æ®è¿ç§»åˆ°SQLiteæ•°æ®åº“ï¼Œæå‡æ€§èƒ½
"""

import sqlite3
import json
from pathlib import Path
import time
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.extract_generic_names import extract_generic_name_and_dosage


class JSONToSQLiteMigrator:
    """JSONåˆ°SQLiteçš„è¿ç§»å·¥å…·"""
    
    def __init__(self, db_path='ontology/data/medical_kg.db'):
        self.db_path = Path(db_path)
        self.conn = None
        self.data_dir = Path('ontology/data')
        
    def create_database(self):
        """åˆ›å»ºæ•°æ®åº“å’Œè¡¨ç»“æ„"""
        print("ğŸ“¦ åˆ›å»ºæ•°æ®åº“ç»“æ„...")
        
        # åˆ é™¤æ—§æ•°æ®åº“
        if self.db_path.exists():
            print(f"âš ï¸  åˆ é™¤æ—§æ•°æ®åº“: {self.db_path}")
            self.db_path.unlink()
        
        self.conn = sqlite3.connect(self.db_path)
        cursor = self.conn.cursor()
        
        # åˆ›å»ºè¡¨
        cursor.executescript('''
            -- å®ä½“è¡¨
            CREATE TABLE entities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                standard_name TEXT NOT NULL,
                type TEXT NOT NULL,  -- Drug, Disease, Gene
                source TEXT,         -- NMPA, ICD-10, TTD
                generic_name TEXT,   -- é€šç”¨åï¼ˆç”¨äºè¯ç‰©ï¼‰
                dosage_form TEXT,    -- å‰‚å‹ï¼ˆç”¨äºè¯ç‰©ï¼‰
                is_generic INTEGER DEFAULT 0,  -- æ˜¯å¦ä¸ºé€šç”¨åï¼ˆ0=åˆ¶å‰‚ï¼Œ1=é€šç”¨åï¼‰
                data TEXT            -- JSONæ ¼å¼å­˜å‚¨å…¶ä»–å±æ€§
            );
            
            CREATE INDEX idx_entities_name ON entities(name);
            CREATE INDEX idx_entities_standard_name ON entities(standard_name);
            CREATE INDEX idx_entities_type ON entities(type);
            CREATE INDEX idx_entities_source ON entities(source);
            CREATE INDEX idx_entities_generic_name ON entities(generic_name);
            CREATE INDEX idx_entities_is_generic ON entities(is_generic);
            
            -- åˆ«åè¡¨ï¼ˆç”¨äºå¿«é€Ÿåˆ«åæŸ¥è¯¢ï¼‰
            CREATE TABLE aliases (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                entity_id INTEGER NOT NULL,
                alias TEXT NOT NULL,
                FOREIGN KEY (entity_id) REFERENCES entities(id) ON DELETE CASCADE
            );
            
            CREATE INDEX idx_aliases_alias ON aliases(alias);
            CREATE INDEX idx_aliases_entity_id ON aliases(entity_id);
            
            -- å…³ç³»è¡¨
            CREATE TABLE relations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_entity_id INTEGER,
                target_entity_id INTEGER,
                relation_type TEXT NOT NULL,  -- targets, treats, causes, etc.
                source_name TEXT,  -- å†—ä½™å­˜å‚¨ï¼ŒåŠ å¿«æŸ¥è¯¢
                target_name TEXT,  -- å†—ä½™å­˜å‚¨ï¼ŒåŠ å¿«æŸ¥è¯¢
                properties TEXT,   -- JSONæ ¼å¼å­˜å‚¨å…³ç³»å±æ€§
                FOREIGN KEY (source_entity_id) REFERENCES entities(id) ON DELETE CASCADE,
                FOREIGN KEY (target_entity_id) REFERENCES entities(id) ON DELETE CASCADE
            );
            
            CREATE INDEX idx_relations_source ON relations(source_entity_id);
            CREATE INDEX idx_relations_target ON relations(target_entity_id);
            CREATE INDEX idx_relations_type ON relations(relation_type);
            CREATE INDEX idx_relations_source_name ON relations(source_name);
            CREATE INDEX idx_relations_target_name ON relations(target_name);
            
            -- å…ƒæ•°æ®è¡¨
            CREATE TABLE metadata (
                key TEXT PRIMARY KEY,
                value TEXT
            );
        ''')
        
        self.conn.commit()
        print("âœ… æ•°æ®åº“ç»“æ„åˆ›å»ºå®Œæˆ")
    
    def migrate_entities_from_unified(self):
        """ä»unified_ontology.jsonè¿ç§»å®ä½“æ•°æ®"""
        unified_file = self.data_dir / 'unified_ontology.json'
        
        if not unified_file.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {unified_file}")
            return {'drugs': 0, 'diseases': 0, 'genes': 0}
        
        print(f"\nğŸ“¥ ä»ç»Ÿä¸€æœ¬ä½“è¿ç§»: {unified_file}")
        
        with open(unified_file, 'r', encoding='utf-8') as f:
            ontology = json.load(f)
        
        entities = ontology.get('entities', {})
        cursor = self.conn.cursor()
        stats = {'drugs': 0, 'diseases': 0, 'genes': 0}
        
        # è¿ç§»è¯ç‰©
        if 'drugs' in entities:
            print(f"\n  å¤„ç†è¯ç‰©æ•°æ®...")
            for name, info in entities['drugs'].items():
                standard_name = info.get('standard_name', name)
                sources = info.get('data_sources', ['Unknown'])
                source = ','.join(sources) if isinstance(sources, list) else sources
                
                data_to_store = {k: v for k, v in info.items() 
                               if k not in ['aliases', 'data_sources', 'standard_name', 'generic_name', 'dosage_form', 'is_generic']}
                
                # æå–é€šç”¨åå’Œå‰‚å‹
                generic_name = info.get('generic_name')
                dosage_form = info.get('dosage_form')
                is_generic = info.get('is_generic', 0)
                
                # å¦‚æœæ²¡æœ‰é€šç”¨åå­—æ®µï¼Œå°è¯•ä»åç§°ä¸­æå–
                if not generic_name:
                    generic_name, dosage_form, is_generic_flag = extract_generic_name_and_dosage(name)
                    is_generic = 1 if is_generic_flag else 0
                
                cursor.execute('''
                    INSERT INTO entities (name, standard_name, type, source, generic_name, dosage_form, is_generic, data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (name, standard_name, 'Drug', source, generic_name, dosage_form, is_generic,
                      json.dumps(data_to_store, ensure_ascii=False)))
                
                entity_id = cursor.lastrowid
                stats['drugs'] += 1
                
                # æ’å…¥åˆ«å
                for alias in info.get('aliases', []):
                    if alias and alias != name:
                        cursor.execute('''
                            INSERT INTO aliases (entity_id, alias) VALUES (?, ?)
                        ''', (entity_id, alias))
                
                if stats['drugs'] % 1000 == 0:
                    self.conn.commit()
                    print(f"    å·²å¤„ç†: {stats['drugs']:,} æ¡...")
            
            self.conn.commit()
            print(f"  âœ… è¯ç‰©: {stats['drugs']:,} æ¡")
        
        # è¿ç§»ç–¾ç—…
        if 'diseases' in entities:
            print(f"\n  å¤„ç†ç–¾ç—…æ•°æ®...")
            for name, info in entities['diseases'].items():
                standard_name = info.get('standard_name', name)
                sources = info.get('data_sources', ['Unknown'])
                source = ','.join(sources) if isinstance(sources, list) else sources
                
                data_to_store = {k: v for k, v in info.items() 
                               if k not in ['aliases', 'data_sources', 'standard_name']}
                
                cursor.execute('''
                    INSERT INTO entities (name, standard_name, type, source, generic_name, dosage_form, is_generic, data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (name, standard_name, 'Disease', source, None, None, 0,
                      json.dumps(data_to_store, ensure_ascii=False)))
                
                entity_id = cursor.lastrowid
                stats['diseases'] += 1
                
                # æ’å…¥åˆ«å
                for alias in info.get('aliases', []):
                    if alias and alias != name:
                        cursor.execute('''
                            INSERT INTO aliases (entity_id, alias) VALUES (?, ?)
                        ''', (entity_id, alias))
                
                if stats['diseases'] % 1000 == 0:
                    self.conn.commit()
                    print(f"    å·²å¤„ç†: {stats['diseases']:,} æ¡...")
            
            self.conn.commit()
            print(f"  âœ… ç–¾ç—…: {stats['diseases']:,} æ¡")
        
        # è¿ç§»åŸºå› 
        if 'genes' in entities:
            print(f"\n  å¤„ç†åŸºå› /é¶ç‚¹æ•°æ®...")
            for name, info in entities['genes'].items():
                standard_name = info.get('standard_name', name)
                sources = info.get('data_sources', ['Unknown'])
                source = ','.join(sources) if isinstance(sources, list) else sources
                
                data_to_store = {k: v for k, v in info.items() 
                               if k not in ['aliases', 'data_sources', 'standard_name']}
                
                cursor.execute('''
                    INSERT INTO entities (name, standard_name, type, source, generic_name, dosage_form, is_generic, data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (name, standard_name, 'Gene', source, None, None, 0,
                      json.dumps(data_to_store, ensure_ascii=False)))
                
                entity_id = cursor.lastrowid
                stats['genes'] += 1
                
                if stats['genes'] % 1000 == 0:
                    self.conn.commit()
                    print(f"    å·²å¤„ç†: {stats['genes']:,} æ¡...")
            
            self.conn.commit()
            print(f"  âœ… åŸºå› : {stats['genes']:,} æ¡")
        
        return stats
    
    def migrate_relations(self):
        """è¿ç§»å…³ç³»æ•°æ®"""
        relations_file = self.data_dir / 'enhanced_relations.json'
        
        if not relations_file.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {relations_file}")
            return 0
        
        print(f"\nğŸ”— è¿ç§»å…³ç³»æ•°æ®: {relations_file}")
        
        with open(relations_file, 'r', encoding='utf-8') as f:
            relations = json.load(f)
        
        cursor = self.conn.cursor()
        
        # åˆ›å»ºåç§°åˆ°IDçš„æ˜ å°„
        print("  æ„å»ºå®ä½“åç§°ç´¢å¼•...")
        cursor.execute('SELECT id, name, standard_name FROM entities')
        name_to_id = {}
        for row in cursor.fetchall():
            entity_id, name, standard_name = row
            name_to_id[name] = entity_id
            if standard_name != name:
                name_to_id[standard_name] = entity_id
        
        total_count = 0
        
        # è¿ç§»å„ç±»å…³ç³»
        relation_types = {
            'target_drug': 'targets',
            'drug_disease': 'treats',
            'target_disease': 'associated_with'
        }
        
        for rel_key, rel_type in relation_types.items():
            if rel_key not in relations:
                continue
            
            print(f"\n  å¤„ç† {rel_type} å…³ç³»...")
            count = 0
            
            for rel in relations[rel_key]:
                # è·å–æºå®ä½“å’Œç›®æ ‡å®ä½“åç§°
                if rel_key == 'target_drug':
                    source_name = rel.get('target_name')
                    target_name = rel.get('drug_name')
                elif rel_key == 'drug_disease':
                    source_name = rel.get('drug_name')
                    target_name = rel.get('disease_id')
                elif rel_key == 'target_disease':
                    source_name = rel.get('target_name')
                    target_name = rel.get('disease_id')
                else:
                    continue
                
                if not source_name or not target_name:
                    continue
                
                # æŸ¥æ‰¾å®ä½“ID
                source_id = name_to_id.get(source_name)
                target_id = name_to_id.get(target_name)
                
                # åˆ é™¤å·²å­˜å‚¨çš„åç§°å­—æ®µ
                properties = {k: v for k, v in rel.items() 
                            if k not in ['target_name', 'drug_name', 'disease_id']}
                
                cursor.execute('''
                    INSERT INTO relations 
                    (source_entity_id, target_entity_id, relation_type, 
                     source_name, target_name, properties)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (source_id, target_id, rel_type, 
                      source_name, target_name, 
                      json.dumps(properties, ensure_ascii=False)))
                
                count += 1
                total_count += 1
                
                if count % 1000 == 0:
                    self.conn.commit()
                    print(f"    å·²å¤„ç†: {count:,} æ¡...")
            
            self.conn.commit()
            print(f"  âœ… {rel_type}: {count:,} æ¡")
        
        return total_count
    
    def save_metadata(self, stats):
        """ä¿å­˜å…ƒæ•°æ®"""
        print("\nğŸ’¾ ä¿å­˜å…ƒæ•°æ®...")
        
        cursor = self.conn.cursor()
        metadata = {
            'version': '1.0.0',
            'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_entities': str(stats['total_entities']),
            'total_relations': str(stats['total_relations']),
            'data_sources': 'NMPA,ICD-10,TTD'
        }
        
        for key, value in metadata.items():
            cursor.execute('''
                INSERT INTO metadata (key, value) VALUES (?, ?)
            ''', (key, value))
        
        self.conn.commit()
        print("âœ… å…ƒæ•°æ®ä¿å­˜å®Œæˆ")
    
    def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        print("\nâš¡ ä¼˜åŒ–æ•°æ®åº“...")
        
        cursor = self.conn.cursor()
        
        # åˆ†æè¡¨å’Œç´¢å¼•
        cursor.execute('ANALYZE')
        
        # æ¸…ç†å’Œå‹ç¼©
        cursor.execute('VACUUM')
        
        self.conn.commit()
        print("âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("  JSON â†’ SQLite æ•°æ®è¿ç§»")
    print("=" * 70)
    
    start_time = time.time()
    
    # åˆ›å»ºè¿ç§»å™¨
    migrator = JSONToSQLiteMigrator()
    
    try:
        # 1. åˆ›å»ºæ•°æ®åº“
        migrator.create_database()
        
        # 2. è¿ç§»å®ä½“æ•°æ®
        entity_stats = migrator.migrate_entities_from_unified()
        
        # 3. è¿ç§»å…³ç³»æ•°æ®
        relation_count = migrator.migrate_relations()
        
        # 4. ä¿å­˜å…ƒæ•°æ®
        stats = {
            'total_entities': sum(entity_stats.values()),
            'total_relations': relation_count,
            **entity_stats
        }
        migrator.save_metadata(stats)
        
        # 5. ä¼˜åŒ–æ•°æ®åº“
        migrator.optimize_database()
        
        # ç»Ÿè®¡ä¿¡æ¯
        elapsed = time.time() - start_time
        db_size = Path(migrator.db_path).stat().st_size / (1024 * 1024)
        
        print("\n" + "=" * 70)
        print("  è¿ç§»å®Œæˆï¼")
        print("=" * 70)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  å®ä½“æ€»æ•°: {stats['total_entities']:,}")
        print(f"    - è¯ç‰©: {stats['drugs']:,}")
        print(f"    - ç–¾ç—…: {stats['diseases']:,}")
        print(f"    - åŸºå› : {stats['genes']:,}")
        print(f"  å…³ç³»æ€»æ•°: {stats['total_relations']:,}")
        print(f"\nğŸ“¦ æ•°æ®åº“æ–‡ä»¶:")
        print(f"  è·¯å¾„: {migrator.db_path}")
        print(f"  å¤§å°: {db_size:.1f} MB")
        print(f"\nâ±ï¸  ç”¨æ—¶: {elapsed:.1f} ç§’")
        
        print("\nâœ… è¿ç§»æˆåŠŸï¼ç°åœ¨å¯ä»¥ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢äº†")
        print("\nä¸‹ä¸€æ­¥:")
        print("  ä½¿ç”¨äº¤äº’å¼æŸ¥è¯¢: python kg_query_db.py")
        
    except Exception as e:
        print(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        migrator.close()


if __name__ == '__main__':
    main()

