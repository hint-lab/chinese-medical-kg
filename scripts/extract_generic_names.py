#!/usr/bin/env python3
"""
æå–è¯å“é€šç”¨å
ä»åˆ¶å‰‚åç§°ä¸­æå–é€šç”¨åå’Œå‰‚å‹
ä¾‹å¦‚: "é˜¿å¸åŒ¹æ—æ³¨å°„æ¶²" â†’ é€šç”¨å: "é˜¿å¸åŒ¹æ—", å‰‚å‹: "æ³¨å°„æ¶²"
"""

import re
import json
from pathlib import Path
from collections import defaultdict


# å¸¸è§å‰‚å‹åˆ—è¡¨
DOSAGE_FORMS = [
    'æ³¨å°„æ¶²', 'æ³¨å°„å‰‚', 'é’ˆå‰‚',
    'ç‰‡', 'ç‰‡å‰‚',
    'èƒ¶å›Š', 'èƒ¶å›Šå‰‚',
    'é¢—ç²’', 'é¢—ç²’å‰‚',
    'æ•£', 'æ•£å‰‚',
    'ä¸¸', 'ä¸¸å‰‚',
    'æ “', 'æ “å‰‚',
    'è½¯è†', 'è½¯è†å‰‚',
    'ä¹³è†', 'ä¹³è†å‰‚',
    'å‡èƒ¶', 'å‡èƒ¶å‰‚',
    'è´´', 'è´´å‰‚',
    'å–·é›¾', 'å–·é›¾å‰‚',
    'å¸å…¥', 'å¸å…¥å‰‚',
    'æ»´çœ¼', 'æ»´çœ¼æ¶²',
    'æ»´è€³', 'æ»´è€³æ¶²',
    'æ»´é¼»', 'æ»´é¼»æ¶²',
    'è‚ æº¶ç‰‡', 'è‚ æº¶èƒ¶å›Š',
    'ç¼“é‡Šç‰‡', 'ç¼“é‡Šèƒ¶å›Š',
    'æ§é‡Šç‰‡', 'æ§é‡Šèƒ¶å›Š',
    'åˆ†æ•£ç‰‡',
    'å’€åš¼ç‰‡',
    'æ³¡è…¾ç‰‡',
    'å£å«ç‰‡',
    'èˆŒä¸‹ç‰‡',
    'è–„è†œè¡£ç‰‡',
    'ç³–è¡£ç‰‡',
    'æº¶æ¶²', 'æº¶æ¶²å‰‚',
    'æ··æ‚¬æ¶²', 'æ··æ‚¬å‰‚',
    'ä¹³å‰‚',
    'ç³–æµ†', 'ç³–æµ†å‰‚',
    'å£æœæ¶²',
    'åˆå‰‚',
]


def extract_generic_name_and_dosage(drug_name: str) -> tuple:
    """
    ä»è¯å“åç§°ä¸­æå–é€šç”¨åå’Œå‰‚å‹
    
    Args:
        drug_name: è¯å“åç§°ï¼Œå¦‚"é˜¿å¸åŒ¹æ—æ³¨å°„æ¶²"
    
    Returns:
        (generic_name, dosage_form, is_generic)
        - generic_name: é€šç”¨åï¼Œå¦‚"é˜¿å¸åŒ¹æ—"
        - dosage_form: å‰‚å‹ï¼Œå¦‚"æ³¨å°„æ¶²"
        - is_generic: æ˜¯å¦ä¸ºé€šç”¨åï¼ˆæ— å‰‚å‹åç¼€ï¼‰
    """
    if not drug_name:
        return drug_name, None, True
    
    # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿å‰‚å‹ï¼ˆå¦‚"è‚ æº¶ç‰‡"åº”è¯¥åœ¨"ç‰‡"ä¹‹å‰ï¼‰
    sorted_forms = sorted(DOSAGE_FORMS, key=len, reverse=True)
    
    # å°è¯•åŒ¹é…å‰‚å‹
    for form in sorted_forms:
        if drug_name.endswith(form):
            generic_name = drug_name[:-len(form)]
            if generic_name:  # ç¡®ä¿æå–åˆ°é€šç”¨å
                return generic_name, form, False
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å‰‚å‹ï¼Œå¯èƒ½æ˜¯é€šç”¨å
    return drug_name, None, True


def analyze_drugs(data_dir='ontology/data'):
    """åˆ†æç°æœ‰è¯ç‰©æ•°æ®ï¼Œæå–é€šç”¨å"""
    data_dir = Path(data_dir)
    
    # åŠ è½½è¯ç‰©æ•°æ®
    drugs_file = data_dir / 'drugs.json'
    if not drugs_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {drugs_file}")
        return
    
    with open(drugs_file, 'r', encoding='utf-8') as f:
        drugs = json.load(f)
    
    print("=" * 70)
    print("  è¯å“é€šç”¨åæå–åˆ†æ")
    print("=" * 70)
    
    # ç»Ÿè®¡
    generic_to_products = defaultdict(list)
    products_with_generic = {}
    generic_only = []
    
    for drug_name, drug_info in drugs.items():
        generic_name, dosage_form, is_generic = extract_generic_name_and_dosage(drug_name)
        
        if is_generic:
            generic_only.append(drug_name)
        else:
            generic_to_products[generic_name].append({
                'product_name': drug_name,
                'dosage_form': dosage_form
            })
            products_with_generic[drug_name] = {
                'generic_name': generic_name,
                'dosage_form': dosage_form
            }
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»è¯ç‰©æ•°: {len(drugs):,}")
    print(f"  é€šç”¨åï¼ˆæ— å‰‚å‹ï¼‰: {len(generic_only):,}")
    print(f"  åˆ¶å‰‚ï¼ˆæœ‰å‰‚å‹ï¼‰: {len(products_with_generic):,}")
    print(f"  é€šç”¨åç§ç±»: {len(generic_to_products):,}")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“‹ ç¤ºä¾‹ï¼ˆå‰10ä¸ªé€šç”¨åï¼‰:")
    for i, (generic, products) in enumerate(list(generic_to_products.items())[:10], 1):
        print(f"\n  {i}. é€šç”¨å: {generic}")
        print(f"     åˆ¶å‰‚æ•°: {len(products)}")
        print(f"     åˆ¶å‰‚åˆ—è¡¨: {', '.join([p['product_name'] for p in products[:5]])}")
        if len(products) > 5:
            print(f"     ... è¿˜æœ‰ {len(products) - 5} ä¸ª")
    
    # ä¿å­˜ç»“æœ
    output_file = data_dir / 'drug_generic_mapping.json'
    mapping = {
        'generic_to_products': {k: v for k, v in generic_to_products.items()},
        'products_with_generic': products_with_generic,
        'generic_only': generic_only,
        'statistics': {
            'total_drugs': len(drugs),
            'generic_count': len(generic_only),
            'product_count': len(products_with_generic),
            'generic_types': len(generic_to_products)
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return mapping


def update_drugs_with_generic_names(data_dir='ontology/data'):
    """æ›´æ–°è¯ç‰©æ•°æ®ï¼Œæ·»åŠ é€šç”¨åå­—æ®µ"""
    data_dir = Path(data_dir)
    
    # åŠ è½½åŸå§‹æ•°æ®
    drugs_file = data_dir / 'drugs.json'
    if not drugs_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {drugs_file}")
        return
    
    with open(drugs_file, 'r', encoding='utf-8') as f:
        drugs = json.load(f)
    
    # åŠ è½½é€šç”¨åæ˜ å°„
    mapping_file = data_dir / 'drug_generic_mapping.json'
    if mapping_file.exists():
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        products_mapping = mapping.get('products_with_generic', {})
    else:
        products_mapping = {}
        for drug_name in drugs.keys():
            generic_name, dosage_form, is_generic = extract_generic_name_and_dosage(drug_name)
            if not is_generic:
                products_mapping[drug_name] = {
                    'generic_name': generic_name,
                    'dosage_form': dosage_form
                }
    
    # æ›´æ–°è¯ç‰©æ•°æ®
    updated_count = 0
    for drug_name, drug_info in drugs.items():
        # æå–é€šç”¨å
        generic_name, dosage_form, is_generic = extract_generic_name_and_dosage(drug_name)
        
        # æ·»åŠ å­—æ®µ
        drug_info['generic_name'] = generic_name
        drug_info['is_generic'] = is_generic
        if dosage_form:
            drug_info['dosage_form'] = dosage_form
        
        # å¦‚æœæ˜¯åˆ¶å‰‚ï¼Œæ·»åŠ é€šç”¨åå…³è”
        if not is_generic and generic_name in drugs:
            if 'related_products' not in drugs[generic_name]:
                drugs[generic_name]['related_products'] = []
            if drug_name not in drugs[generic_name]['related_products']:
                drugs[generic_name]['related_products'].append(drug_name)
        
        updated_count += 1
    
    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
    output_file = data_dir / 'drugs_with_generic.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(drugs, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²æ›´æ–° {updated_count:,} ä¸ªè¯ç‰©ï¼Œæ·»åŠ é€šç”¨åå­—æ®µ")
    print(f"âœ… ä¿å­˜åˆ°: {output_file}")
    
    return drugs


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æå–è¯å“é€šç”¨å')
    parser.add_argument('--analyze', action='store_true', help='åˆ†æç°æœ‰æ•°æ®')
    parser.add_argument('--update', action='store_true', help='æ›´æ–°è¯ç‰©æ•°æ®ï¼Œæ·»åŠ é€šç”¨åå­—æ®µ')
    parser.add_argument('--data-dir', default='ontology/data', help='æ•°æ®ç›®å½•')
    
    args = parser.parse_args()
    
    if args.analyze:
        analyze_drugs(args.data_dir)
    
    if args.update:
        update_drugs_with_generic_names(args.data_dir)
    
    if not args.analyze and not args.update:
        # é»˜è®¤æ‰§è¡Œåˆ†æå’Œæ›´æ–°
        print("æ‰§è¡Œåˆ†æå’Œæ›´æ–°...")
        analyze_drugs(args.data_dir)
        print("\n" + "=" * 70)
        update_drugs_with_generic_names(args.data_dir)


if __name__ == '__main__':
    main()

